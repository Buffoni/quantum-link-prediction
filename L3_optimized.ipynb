{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cdd3001",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported TrainSet2014_3.pkl\n",
      "\n",
      "Built adjacency matrix with:\n",
      " -  64719  vertices\n",
      " -  2278611  edges\n",
      "\n",
      "Found  27230  unconnected nodes.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import datetime\n",
    "\n",
    "#### IMPORT DATA ####\n",
    "\n",
    "data_source = 'TrainSet2014_3.pkl'\n",
    "#data_source = 'CompetitionSet2017_3.pkl'\n",
    "\n",
    "full_dynamic_graph_sparse, unconnected_vertex_pairs, year_start, years_delta = pickle.load(open( data_source, \"rb\" ) )\n",
    "\n",
    "NUM_OF_VERTICES = 64719 # number of vertices of the semantic net\n",
    "NUM_OF_EDGES    = full_dynamic_graph_sparse[:, 0].size\n",
    "\n",
    "#### BUILD ADJACENCY ####\n",
    "\n",
    "# The concatenation is used to produce a symmetric adjacency matrix\n",
    "data_rows = np.concatenate([full_dynamic_graph_sparse[:, 0], full_dynamic_graph_sparse[:, 1]])\n",
    "data_cols = np.concatenate([full_dynamic_graph_sparse[:, 1], full_dynamic_graph_sparse[:, 0]])\n",
    "data_ones = np.ones(len(data_rows), np.uint32)\n",
    "\n",
    "adjM = ss.csr_matrix((data_ones, (data_rows, data_cols)), shape=(NUM_OF_VERTICES, NUM_OF_VERTICES))\n",
    "\n",
    "#### BUILD DEGREE VECTOR ####\n",
    "\n",
    "degree_vec = np.asarray(adjM.sum(1)).flatten()\n",
    "\n",
    "print(\"Imported\", data_source)\n",
    "print(\"\\nBuilt adjacency matrix with:\")\n",
    "print(\" - \", NUM_OF_VERTICES, \" vertices\")\n",
    "print(\" - \", NUM_OF_EDGES, \" edges\\n\")\n",
    "print(\"Found \", np.count_nonzero(degree_vec == 0),\" unconnected nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf828d4",
   "metadata": {},
   "source": [
    "# Analyzing the links to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88bc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1.000.000 new links:\n",
      "265966  are between two vertices with k = 0 -> ordered randomly;\n",
      "500060  are between one vertex with k = 0 and one with k > 0 -> ordered by preferential attachment;\n",
      "233974  are between two vertices with k > 0 -> ordered by network based predictions;\n"
     ]
    }
   ],
   "source": [
    "pred_degree0 = degree_vec[unconnected_vertex_pairs[:,0]]\n",
    "pred_degree1 = degree_vec[unconnected_vertex_pairs[:,1]]\n",
    "\n",
    "# Counts how many links between two nodes with k = 0:\n",
    "k0 = np.count_nonzero((pred_degree0 + pred_degree1) == 0)\n",
    "# Counts how many links between one nodes with k = 0 and one k > 0:\n",
    "k1 = np.count_nonzero(pred_degree0 * pred_degree1 == 0) - np.count_nonzero((pred_degree0 + pred_degree1) == 0)\n",
    "# Counts how many links between two nodes with k > 0:\n",
    "k2 = np.count_nonzero((pred_degree0 * pred_degree1) > 0)\n",
    "\n",
    "print(\"Out of 1.000.000 new links:\")\n",
    "print(k0,\" are between two vertices with k = 0 -> ordered randomly;\")\n",
    "print(k1,\" are between one vertex with k = 0 and one with k > 0 -> ordered by preferential attachment;\")\n",
    "print(k2,\" are between two vertices with k > 0 -> ordered by network based predictions;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526ca26",
   "metadata": {},
   "source": [
    "# L3 Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75ff0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l3_method(adjM, links_to_score):\n",
    "    # adjM is assumed to be a sparse matrix\n",
    "    # links_to_score is a list of pairs of nodes (v1, v2)\n",
    "    # returns an ordered list of those pairs by their index\n",
    "    # running from 0 to 999.999\n",
    "    \n",
    "    # Building D^-1/2.\n",
    "    degree_vec = np.asarray(adjM.sum(1)).flatten()\n",
    "    degree_vec = np.where(degree_vec == 0, 1, degree_vec) # to avoid division by 0\n",
    "    sqrt_degree_vec    = 1/np.sqrt(degree_vec)\n",
    "    sqrt_degree_matrix = ss.diags(sqrt_degree_vec, 0)\n",
    "\n",
    "    # Computing Ãƒ\n",
    "    adjM_tilde = sqrt_degree_matrix*adjM*sqrt_degree_matrix;\n",
    "    \n",
    "    # Select rows and columns of A corresponding to links to score\n",
    "    rows = np.unique(links_to_score[:,0])\n",
    "    cols = np.unique(links_to_score[:,1])\n",
    "    \n",
    "    # Computing P\n",
    "    p_matrix = adjM[rows,:]*adjM_tilde*adjM[:,cols];\n",
    "    \n",
    "    # Note that p_matrix has dimensions size(rows)*size(rows). As it happens in the data from the\n",
    "    # competition the indices in unconnected_vertex_pairs actually only go up to size(rows), which means\n",
    "    # that we can use those same indices to call values from p_matrix. If that were not the case,\n",
    "    # we would have to implement a dictionary here to correct the indices before calling values from\n",
    "    # p_matrix. I checked and in the competition data the same thing happens, and so I didn't implement\n",
    "    # the dictionary.\n",
    "    \n",
    "    # Ordering relevannt scores\n",
    "    score_list = np.array(p_matrix[links_to_score[:,0], links_to_score[:,1]]).flatten()\n",
    "    sorted_predictions = np.argsort(-1.0*score_list)\n",
    "    \n",
    "    return sorted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04aab2bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:05:58.015341\n",
      "Computing L3 scores...\n",
      "Done!\n",
      "16:06:35.565568\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().time())\n",
    "\n",
    "print(\"Computing L3 scores...\")\n",
    "\n",
    "sorted_predictions = l3_method(adjM, unconnected_vertex_pairs)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7707b",
   "metadata": {},
   "source": [
    "# AUC (for 2014 -> 2017 predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "583c2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_ROC(data_vertex_pairs, data_solution, show_plot = False):\n",
    "    data_solution = np.array(data_solution)\n",
    "    data_vertex_pairs_sorted = data_solution[data_vertex_pairs]\n",
    "    \n",
    "    xpos=[0]\n",
    "    ypos=[0]\n",
    "    ROC_vals=[]\n",
    "    for ii in range(len(data_vertex_pairs_sorted)):\n",
    "        if data_vertex_pairs_sorted[ii]==1:\n",
    "            xpos.append(xpos[-1])\n",
    "            ypos.append(ypos[-1]+1)\n",
    "        if data_vertex_pairs_sorted[ii]==0:\n",
    "            xpos.append(xpos[-1]+1)\n",
    "            ypos.append(ypos[-1])      \n",
    "            ROC_vals.append(ypos[-1])\n",
    "    \n",
    "    ROC_vals=np.array(ROC_vals)/max(ypos)\n",
    "    ypos=np.array(ypos)/max(ypos)\n",
    "    xpos=np.array(xpos)/max(xpos)\n",
    "    \n",
    "    if show_plot == True:\n",
    "        plt.plot(xpos, ypos)\n",
    "        plt.show()\n",
    "    \n",
    "    AUC = sum(ROC_vals)/len(ROC_vals)\n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3eef580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Curve for Evaluation:  0.6928035870074668\n"
     ]
    }
   ],
   "source": [
    "with open('TrainSet2014_3_solution.pkl', \"rb\" ) as pkl_file:\n",
    "        unconnected_vertex_pairs_solution = pickle.load(pkl_file)\n",
    "    \n",
    "AUC = calculate_ROC(sorted_predictions, np.array(unconnected_vertex_pairs_solution))\n",
    "\n",
    "print('Area Under Curve for Evaluation: ', AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3aa21c",
   "metadata": {},
   "source": [
    "# Exporting a submission file (for 2017 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the results for submission.\n",
    "submit_file = \"test_submission_01.json\"\n",
    "\n",
    "all_idx_list_float=list(map(float, sorted_predictions))\n",
    "with open(submit_file, \"w\", encoding=\"utf8\") as json_file:\n",
    "    json.dump(all_idx_list_float, json_file)\n",
    "    \n",
    "print(\"Solution stored as \"+submit_file+\".\\nLooking forward to your submission.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
