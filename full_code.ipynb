{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import datetime\n",
    "\n",
    "#### IMPORT DATA ####\n",
    "\n",
    "data_source = 'TrainSet2014_3.pkl'\n",
    "#data_source = 'CompetitionSet2017_3.pkl'\n",
    "\n",
    "full_dynamic_graph_sparse, unconnected_vertex_pairs, year_start, years_delta = pickle.load(open( data_source, \"rb\" ) )\n",
    "\n",
    "NUM_OF_VERTICES = 64719 # number of vertices of the semantic net\n",
    "NUM_OF_EDGES    = full_dynamic_graph_sparse[:, 0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building unweighted adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported TrainSet2014_3.pkl\n",
      "\n",
      "Built adjacency matrix with:\n",
      " -  64719  vertices\n",
      " -  2278611  edges\n",
      "\n",
      "Found  27230  unconnected nodes.\n"
     ]
    }
   ],
   "source": [
    "# The concatenation is used to produce a symmetric adjacency matrix\n",
    "data_rows = np.concatenate([full_dynamic_graph_sparse[:, 0], full_dynamic_graph_sparse[:, 1]])\n",
    "data_cols = np.concatenate([full_dynamic_graph_sparse[:, 1], full_dynamic_graph_sparse[:, 0]])\n",
    "data_ones = np.ones(len(data_rows), np.uint32)\n",
    "\n",
    "adjM = ss.csr_matrix((data_ones, (data_rows, data_cols)), shape=(NUM_OF_VERTICES, NUM_OF_VERTICES))\n",
    "\n",
    "#### BUILD DEGREE VECTOR ####\n",
    "\n",
    "degree_vec = np.asarray(adjM.sum(1)).flatten()\n",
    "\n",
    "print(\"Imported\", data_source)\n",
    "print(\"\\nBuilt adjacency matrix with:\")\n",
    "print(\" - \", NUM_OF_VERTICES, \" vertices\")\n",
    "print(\" - \", NUM_OF_EDGES, \" edges\\n\")\n",
    "print(\"Found \", np.count_nonzero(degree_vec == 0),\" unconnected nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the links to be predicted\n",
    "\n",
    "In this challenge we are tasked with ordering a list of 1 million pairs of nodes according to which pairs are most likely to appear. To get a better intuition on how to build our model, it is useful to do some preliminary analysis on the network. We start by checking how each pair of nodes is currently connected to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1.000.000 new links:\n",
      "265966  are between two vertices with k = 0;\n",
      "500060  are between one vertex with k = 0 and one with k > 0;\n",
      "233974  are between two vertices with k > 0s;\n"
     ]
    }
   ],
   "source": [
    "pred_degree0 = degree_vec[unconnected_vertex_pairs[:,0]]\n",
    "pred_degree1 = degree_vec[unconnected_vertex_pairs[:,1]]\n",
    "\n",
    "# Counts how many links between two nodes with k = 0:\n",
    "k0 = np.count_nonzero((pred_degree0 + pred_degree1) == 0)\n",
    "# Counts how many links between one nodes with k = 0 and one k > 0:\n",
    "k1 = np.count_nonzero(pred_degree0 * pred_degree1 == 0) - np.count_nonzero((pred_degree0 + pred_degree1) == 0)\n",
    "# Counts how many links between two nodes with k > 0:\n",
    "k2 = np.count_nonzero((pred_degree0 * pred_degree1) > 0)\n",
    "\n",
    "print(\"Out of 1.000.000 new links:\")\n",
    "print(k0,\" are between two vertices with k = 0;\")\n",
    "print(k1,\" are between one vertex with k = 0 and one with k > 0;\")\n",
    "print(k2,\" are between two vertices with k > 0s;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both in the training and competition data there is a significant number of links to be predicted between pairs where at least one of the nodes is disconnected from the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining simple prediction methods\n",
    "\n",
    "### L3 Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l3_method(adjM, links_to_score):\n",
    "    # adjM is assumed to be a sparse matrix\n",
    "    # links_to_score is a list of pairs of nodes (v1, v2)\n",
    "    # returns a list of scores for each pair in links_to_score\n",
    "    \n",
    "    # Building D^-1/2.\n",
    "    degree_vec = np.asarray(adjM.sum(1)).flatten()\n",
    "    degree_vec = np.where(degree_vec == 0, 1, degree_vec) # to avoid division by 0\n",
    "    sqrt_degree_vec    = 1/np.sqrt(degree_vec)\n",
    "    sqrt_degree_matrix = ss.diags(sqrt_degree_vec, 0)\n",
    "\n",
    "    # Computing Ã\n",
    "    adjM_tilde = sqrt_degree_matrix*adjM*sqrt_degree_matrix;\n",
    "    \n",
    "    # Select rows and columns of A corresponding to links to score\n",
    "    rows = np.unique(links_to_score[:,0])\n",
    "    cols = np.unique(links_to_score[:,1])\n",
    "    \n",
    "    # Computing P\n",
    "    p_matrix = (adjM[rows,:]*adjM_tilde*adjM[:,cols]); #adjM*\n",
    "    \n",
    "    # Note that p_matrix has dimensions size(rows)*size(rows). As it happens in the data from the\n",
    "    # competition the indices in unconnected_vertex_pairs actually only go up to size(rows), which means\n",
    "    # that we can use those same indices to call values from p_matrix. If that were not the case,\n",
    "    # we would have to implement a dictionary here to correct the indices before calling values from\n",
    "    # p_matrix. I checked and in the competition data the same thing happens, and so I didn't implement\n",
    "    # the dictionary.\n",
    "    \n",
    "    # Ordering relevant scores\n",
    "    score_list = np.array(p_matrix[links_to_score[:,0], links_to_score[:,1]]).flatten()\n",
    "    \n",
    "    return score_list/max(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_method(adjM, links_to_score):\n",
    "    # adjM is assumed to be a sparse matrix\n",
    "    # links_to_score is a list of pairs of nodes (v1, v2)\n",
    "    # returns a list of scores for each pair in links_to_score\n",
    "    \n",
    "    # Select rows and columns of A corresponding to links to score\n",
    "    rows = np.unique(links_to_score[:,0])\n",
    "    cols = np.unique(links_to_score[:,1])\n",
    "    \n",
    "    # Computing P\n",
    "    p_matrix = adjM[rows,:]*adjM[:,cols];\n",
    "    \n",
    "    # Note that p_matrix has dimensions size(rows)*size(rows). As it happens in the data from the\n",
    "    # competition the indices in unconnected_vertex_pairs actually only go up to size(rows), which means\n",
    "    # that we can use those same indices to call values from p_matrix. If that were not the case,\n",
    "    # we would have to implement a dictionary here to correct the indices before calling values from\n",
    "    # p_matrix. I checked and in the competition data the same thing happens, and so I didn't implement\n",
    "    # the dictionary.\n",
    "    \n",
    "    # Ordering relevant scores\n",
    "    score_list = np.array(p_matrix[links_to_score[:,0], links_to_score[:,1]]).flatten()\n",
    "    \n",
    "    return score_list/max(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferential Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pa_method(adjM, links_to_score):\n",
    "    # adjM is assumed to be a sparse matrix\n",
    "    # links_to_score is a list of pairs of nodes (v1, v2)\n",
    "    # returns a list of scores for each pair in links_to_score\n",
    "    \n",
    "    degree_vec = np.asarray(adjM.sum(1)).flatten()\n",
    "    \n",
    "    score_list = degree_vec[links_to_score[:,0]] + degree_vec[links_to_score[:,1]]\n",
    "    \n",
    "    return score_list/max(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the methods on the training data\n",
    "\n",
    "Skip to the end if running on the competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:54:23.503193\n",
      "Computing L3 scores...\n",
      "Done!\n",
      "Computing CN scores...\n",
      "Done!\n",
      "Computing PA scores...\n",
      "Done!\n",
      "17:54:26.882814\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().time())\n",
    "\n",
    "print(\"Computing L3 scores...\")\n",
    "\n",
    "score_list_l3 = l3_method(adjM, unconnected_vertex_pairs)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Computing CN scores...\")\n",
    "\n",
    "score_list_cn = cn_method(adjM, unconnected_vertex_pairs)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Computing PA scores...\")\n",
    "\n",
    "score_list_pa = pa_method(adjM, unconnected_vertex_pairs)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC function (for 2014 -> 2017 predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_ROC(data_vertex_pairs, data_solution, show_plot = False):\n",
    "    data_solution = np.array(data_solution)\n",
    "    data_vertex_pairs_sorted = data_solution[data_vertex_pairs]\n",
    "    \n",
    "    xpos=[0]\n",
    "    ypos=[0]\n",
    "    ROC_vals=[]\n",
    "    for ii in range(len(data_vertex_pairs_sorted)):\n",
    "        if data_vertex_pairs_sorted[ii]==1:\n",
    "            xpos.append(xpos[-1])\n",
    "            ypos.append(ypos[-1]+1)\n",
    "        if data_vertex_pairs_sorted[ii]==0:\n",
    "            xpos.append(xpos[-1]+1)\n",
    "            ypos.append(ypos[-1])      \n",
    "            ROC_vals.append(ypos[-1])\n",
    "    \n",
    "    ROC_vals=np.array(ROC_vals)/max(ypos)\n",
    "    ypos=np.array(ypos)/max(ypos)\n",
    "    xpos=np.array(xpos)/max(xpos)\n",
    "    \n",
    "    if show_plot == True:\n",
    "        plt.plot(xpos, ypos)\n",
    "        plt.show()\n",
    "    \n",
    "    AUC = sum(ROC_vals)/len(ROC_vals)\n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data AUC:\n",
      "\n",
      "Using tutorial AUC function:\n",
      "- CN Method:  0.6940861916230197\n",
      "- PA Method:  0.8038421610708972\n",
      "\n",
      "Using scikit-learn AUC function, which is much faster:\n",
      "- CN Method:  0.6890970042220121\n",
      "- PA Method:  0.8029338991694484\n"
     ]
    }
   ],
   "source": [
    "# Besides the function above we can also use the one from scikit-learn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with open('TrainSet2014_3_solution.pkl', \"rb\" ) as pkl_file:\n",
    "        unconnected_vertex_pairs_solution = pickle.load(pkl_file)\n",
    "\n",
    "ground_truth = np.array(unconnected_vertex_pairs_solution)\n",
    "\n",
    "# The tutorial AUC function uses the array of sorted indices:\n",
    "sorted_predictions_l3 = np.argsort(-1.0*score_list_l3)\n",
    "sorted_predictions_cn = np.argsort(-1.0*score_list_cn)\n",
    "sorted_predictions_pa = np.argsort(-1.0*score_list_pa)\n",
    "\n",
    "AUC_L3 = calculate_ROC(sorted_predictions_l3, ground_truth, False)\n",
    "AUC_CN = calculate_ROC(sorted_predictions_cn, ground_truth, False)\n",
    "AUC_PA = calculate_ROC(sorted_predictions_pa, ground_truth, False)\n",
    "\n",
    "# The scikit-learn AUC function uses the array of scores directly:\n",
    "AUC_L3_sk = roc_auc_score(ground_truth, score_list_l3)\n",
    "AUC_CN_sk = roc_auc_score(ground_truth, score_list_cn)\n",
    "AUC_PA_sk = roc_auc_score(ground_truth, score_list_pa)\n",
    "\n",
    "print(\"Training data AUC:\\n\")\n",
    "print(\"Using tutorial AUC function:\")\n",
    "print(\"- L3 Method: \", AUC_L3)\n",
    "print(\"- CN Method: \", AUC_CN)\n",
    "print(\"- PA Method: \", AUC_PA)\n",
    "print(\"\\nUsing scikit-learn AUC function, which is much faster:\")\n",
    "print(\"- L3 Method: \", AUC_L3_sk)\n",
    "print(\"- CN Method: \", AUC_CN_sk)\n",
    "print(\"- PA Method: \", AUC_PA_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results obtained we can conclude two things:\n",
    "- There is no difference between using the second or third power of the adjacency matrix. This indicates the path-based structures in the network are not very well defined, and both methods are identifying similar structures. We should also note that in the training data only about 25% of the links to be predicted are between two nodes with k > 0, which are the only pairs that get scored by powers of the adjacency matrix.\n",
    "\n",
    "- Following the previous point and our analysis of the degree distribution, it makes sense that the preferential attachment method is the one with the best performance. Using that method our scores are directly proportional to the degree of the nodes, which is in line with the heavy scale-freeness we observed. Furthermore, given that 50% of the links to be scored in the training data are between a pair of nodes with k = 0 and k > 0, this method has an advantage by also including those pairs in the ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining results\n",
    "\n",
    "Let us now see if we can get even better results by combining two of the previous methods with a greedy parameter search. We considered the combination of Common Neighbours with Preferential Attachment. We already saw that PA alone is the best performing method, but this method is completely independent of the node distance inside the network. We now test if the addition of a contribution given by the number of common neighbours is able to improve over PA. We define:\n",
    "\n",
    "$$scores = a \\times scores_\\text{CN} + (1-a) \\times scores_\\text{PA}$$\n",
    "\n",
    "And search for the optimal \"a\" between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max AUC 0.8085615450037189 for a = 0.93\n"
     ]
    }
   ],
   "source": [
    "step   = 0.01\n",
    "a_list = np.arange(0, 1 + step, step)\n",
    "AUC_save = []\n",
    "\n",
    "for a in a_list:\n",
    "    combined_scores = a*score_list_cn + (1-a)*score_list_pa\n",
    "    AUC = roc_auc_score(ground_truth, combined_scores)\n",
    "    AUC_save.append([a, AUC])\n",
    "\n",
    "AUC_save = np.array(AUC_save)\n",
    "\n",
    "max_AUC_pos = AUC_save[:,1].argmax()\n",
    "\n",
    "print(\"Found max AUC\",AUC_save[max_AUC_pos, 1],\"for a =\",AUC_save[max_AUC_pos, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, adding a contribution of CN over the PA scores gets us a slight increase in the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a time-weighted adjacency matrix\n",
    "\n",
    "So far we have completely ignored the time stamps attached to each link in the data. These time stamps correspond to the date when each link was added to the network, counted as the total number of days since a reference date. In order to include this information in the model, we chose to use this information directly as a weight on each link. We consider the following hypothesis:\n",
    "\n",
    "- The most important links are the very old ones and the very recent ones. The reasoning is that very old links correspond to early connections between concepts, which have had a lot of time to mature and become rooted in the field. The very recent links correspond to the latest connections between concepts, corresponding to the current hot-topic being studied in the field.\n",
    "\n",
    "In order to quantify and test this hypothesis we normalized the time stamp to a value t between 0 and 1, and tested different functions f(t) producing weights in the links. With a bit of manual optimization we found a polynomial function producing the best results, which indeed follows our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the time-stamps\n",
    "times = full_dynamic_graph_sparse[:, 2]\n",
    "t     = times/times.max()\n",
    "\n",
    "# Polynomial function of the time-stamps\n",
    "times_norm = 0.5+(2*t-1)**4 \n",
    "times_norm = np.concatenate([times_norm, times_norm])\n",
    "\n",
    "# Building the weighted adjacency matrix\n",
    "adjM_weighted = ss.csr_matrix((times_norm, (data_rows, data_cols)), shape=(NUM_OF_VERTICES, NUM_OF_VERTICES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply need to run the previous methods with this modified input. Let's test the PA method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PA scores...\n",
      "Using scikit-learn AUC function:\n",
      "- PA Method:  0.8096718961745439\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing PA scores...\")\n",
    "\n",
    "score_list_pa_time = pa_method(adjM_weighted, unconnected_vertex_pairs)\n",
    "\n",
    "AUC_PA_sk_time = roc_auc_score(ground_truth, score_list_pa_time)\n",
    "\n",
    "print(\"Using scikit-learn AUC function:\")\n",
    "print(\"- PA Method: \", AUC_PA_sk_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time-weights also improve the results on the PA method. Now we test the previous greedy search with time weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max AUC 0.8146767284707327 for a = 0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "score_list_cn_time = cn_method(adjM_weighted, unconnected_vertex_pairs)\n",
    "\n",
    "step   = 0.01\n",
    "a_list = np.arange(0, 1 + step, step)\n",
    "AUC_save = []\n",
    "\n",
    "for a in a_list:\n",
    "    combined_scores_time = a*score_list_cn_time + (1-a)*score_list_pa_time\n",
    "    AUC = roc_auc_score(ground_truth, combined_scores_time)\n",
    "    AUC_save.append([a, AUC])\n",
    "\n",
    "AUC_save = np.array(AUC_save)\n",
    "\n",
    "max_AUC_pos = AUC_save[:,1].argmax()\n",
    "\n",
    "print(\"Found max AUC\",AUC_save[max_AUC_pos, 1],\"for a =\",AUC_save[max_AUC_pos, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both the time weights and the combined CN and PA method, we get our best result yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on the competition data (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The concatenation is used to produce a symmetric adjacency matrix\n",
    "data_rows = np.concatenate([full_dynamic_graph_sparse[:, 0], full_dynamic_graph_sparse[:, 1]])\n",
    "data_cols = np.concatenate([full_dynamic_graph_sparse[:, 1], full_dynamic_graph_sparse[:, 0]])\n",
    "data_ones = np.ones(len(data_rows), np.uint32)\n",
    "\n",
    "# Normalizing the time-stamps\n",
    "times = full_dynamic_graph_sparse[:, 2]\n",
    "t     = times/times.max()\n",
    "\n",
    "# Polynomial function of the time-stamps\n",
    "times_norm = 0.5+(2*t-1)**4 \n",
    "times_norm = np.concatenate([times_norm, times_norm])\n",
    "\n",
    "# Building unweighted adjacency matrix\n",
    "adjM = ss.csr_matrix((data_ones, (data_rows, data_cols)), shape=(NUM_OF_VERTICES, NUM_OF_VERTICES))\n",
    "\n",
    "# Building the weighted adjacency matrix\n",
    "adjM_weighted = ss.csr_matrix((times_norm, (data_rows, data_cols)), shape=(NUM_OF_VERTICES, NUM_OF_VERTICES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_l3 = l3_method(adjM, unconnected_vertex_pairs)\n",
    "score_list_cn = cn_method(adjM, unconnected_vertex_pairs)\n",
    "score_list_pa = pa_method(adjM, unconnected_vertex_pairs)\n",
    "\n",
    "score_list_cn_time = cn_method(adjM_weighted, unconnected_vertex_pairs)\n",
    "score_list_pa_time = pa_method(adjM_weighted, unconnected_vertex_pairs)\n",
    "\n",
    "a = 0.94\n",
    "\n",
    "combined_scores_time = a*score_list_cn_time + (1-a)*score_list_pa_time\n",
    "\n",
    "sorted_prediction = np.argsort(-1.0*combined_scores_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Current submissions done:\n",
    "# Bacalhau à Brás - L3 method unweighted\n",
    "# Bacalhau com Todos - PA method unweighted\n",
    "# Bacalhau à Gomes de Sá - CN + PA method unweighted\n",
    "# Bacalhau à Lagareiro - CN + PA method time-weighted - still to be submitted, probably best one yet\n",
    "\n",
    "# Save the results for submission.\n",
    "submit_file = \"bacalhau_a_lagareiro.json\"\n",
    "\n",
    "all_idx_list_float=list(map(float, sorted_predictions))\n",
    "with open(submit_file, \"w\", encoding=\"utf8\") as json_file:\n",
    "    json.dump(all_idx_list_float, json_file)\n",
    "    \n",
    "print(\"Solution stored as \"+submit_file+\".\\nLooking forward to your submission.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
